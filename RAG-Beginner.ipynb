{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# LangChain and AWS Bedrock imports\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_aws.chat_models import ChatBedrockConverse\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "disable_streaming='tool_calling' client=<botocore.client.BedrockRuntime object at 0x791d3d32f310> model_id='us.meta.llama3-1-70b-instruct-v1:0' region_name='us-east-1' provider='us' supports_tool_choice_values=()\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatBedrockConverse(model=\"us.meta.llama3-1-70b-instruct-v1:0\", region_name=\"us-east-1\")\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s).\n",
      "Document metadata: {'source': 'https://blog.langchain.dev/generating-usable-text-with-ai/', 'title': 'Generating Usable Text with AI', 'language': 'en'}\n",
      "Page content length: 14929 characters.\n",
      "Document content (excerpt):\n",
      "   Generating Usable Text with AI\n",
      "Skip to content                 Case Studies     In the Loop     LangChain\n",
      "Docs     Changelog      Sign in Subscribe                    Generating Usable\n",
      "Text with AI  By LangChain 9 min read Feb 5, 2024      Editor's Note: This post\n",
      "was written by Mutt Data through LangChain's Partner\n",
      "Program.IntroductionOverviewIn our previous discussions, we not only delved into\n",
      "the challenges of implementing Generative AI applications in general but also\n",
      "explored effective mitigation strategies for image generation problems. Now,\n",
      "it's time to shift our focus to the unique set of challenges that arise when\n",
      "generating text.In this blog post, we provide a concise overview of these\n",
      "challenges while sharing some insights from our experiences utilizing Large\n",
      "Language Models (LLMs). To lay the groundwork for our exploration, we will first\n",
      "introduce some fundamental concepts such as prompts and prompt engineering.\n",
      "Let’s go right to it!PromptsIf you have been involved in the Generative AI world\n",
      "in the last months, you have probably heard about prompts. Prompts are specific\n",
      "user-provided inputs that guide LLMs to generate an appropriate response\n",
      "tailored to a given task. As well as we expect some kind of order in the\n",
      "sections of a presentation (introduction, discussion, solution, and conclusion),\n",
      "LLMs work much better if we follow some structure in our prompts. Moreover, this\n",
      "structure can greatly vary depending on the task we want to perform. To\n",
      "illustrate, some common tasks supported by LLMs include: classification,\n",
      "question-answer, summarization, code generation, and reasoning. It's essential\n",
      "to adapt the structure to the specific requirements and particularities of each\n",
      "type of task when crafting prompts for these diverse tasks.Prompt EngineeringThe\n",
      "practice of optimizing input prompts by selecting appropriate words, phrases,\n",
      "sentences, punctuation, and separator characters to effectively use LLMs, is\n",
      "known as prompt engineering. In other words, prompt engineering is the art of\n",
      "communicating with an LLM in a manner that aligns with its expected\n",
      "understanding and enhances its performance.An example of a well formed prompt in\n",
      "case of Summarization is attached belowWell formed prompt example. Extracted\n",
      "from docs.aws.amazon.comOn the other hand if your use case is Classification,\n",
      "including some examples (input-response pairs) for each category after the\n",
      "contextual information, could be really helpful. This is known as few-shot\n",
      "prompting and could also apply to more complex tasks such as Question-answer.\n",
      "However, the more complex the task, the more examples that will be needed to\n",
      "improve performance.Another powerful technique is to ask the LLM to reason and\n",
      "explain prior to giving the final answer, also known as “step by step”\n",
      "reasoning. The essential concept behind this relies on the autoregressive nature\n",
      "of these models, which means that each predicted word influences the generation\n",
      "of the next element in the sequence. Please find more details about this\n",
      "here. Last but not least we could consider refining our prompts with modifiers,\n",
      "such as including details about the input data, specifying output format or\n",
      "simply encouraging the LLM at the end of the prompt. Amazon Web Services\n",
      "provides further details about this.Navigating Challenges and Solutions1) Less\n",
      "is MoreChallenge: When using LLMs we’re talking to a program, not a human.Unlike\n",
      "human conversations where we can be somewhat messy and correct ourselves,\n",
      "providing complex or controversial instructions to an LLM can lead to\n",
      "inconsistent and incorrect responses. Clear and straightforward instructions are\n",
      "crucial for helping the LLM focus on relevant information and produce accurate\n",
      "results.Solution: Clear, concise, and smart prompting.Be as clear as possible\n",
      "when crafting prompts. Also known as prompt engineering, selecting precise words\n",
      "and phrases for clear and concise instructions is key. You can also explore\n",
      "prompt templates for similar use cases to discover effective structures for\n",
      "certain tasks.Example: Let’s consider two prompts for summarizing a food product\n",
      "description from a lengthy description into a concise title and caption for\n",
      "packaging.Prompt 1: Please analyze the extensive details provided about this\n",
      "food product, including the ingredients, benefits, instructions, and customer\n",
      "reviews. Extract the most crucial information, taking into account various\n",
      "aspects such as nutritional value, taste, user experiences, and overall market\n",
      "trends. Create a concise summary suitable for product packaging, striking a\n",
      "balance between brevity and capturing the product's multifaceted qualities.\n",
      "Also, ensure that the summary caters to diverse consumer preferences and aligns\n",
      "with current industry standards. It should be versatile enough to meet the\n",
      "expectations of both seasoned food enthusiasts and those new to culinary\n",
      "experiences. Additionally, incorporate any notable advancements in food science\n",
      "and technology to highlight the product's innovative features.Prompt 2:\n",
      " Summarize essential details for a food product, emphasizing ingredients,\n",
      "benefits, and user experiences. Create a brief yet compelling title and caption\n",
      "suitable for product packaging, ensuring its appeal to a broad audience and\n",
      "highlighting any innovative features. Emphasize clarity and brevity in the\n",
      "summary, taking into consideration the varied preferences and experiences of\n",
      "food enthusiasts.Which one describes the task in a clearer way?2) Divide &\n",
      "ConquerChallenge: LLMs excel at tackling some specific tasks, but we should not\n",
      "overwhelm them by asking to do a bunch of them at the same time! Solution: Break\n",
      "down complex tasks into simpler ones by using straightforward prompts and then\n",
      "gather the results.Example: When recommending movies from a long list of options\n",
      "to users, we found that the accuracy of correct recommendations improved\n",
      "significantly by asking if the movies were suitable one by one instead of\n",
      "providing long lists of movies altogether. Check the following two\n",
      "prompts:Prompt 1: You are a language model assisting a web platform in\n",
      "recommending movies to users based on their preferences. Your objective is to\n",
      "present the movies attractively and sorted by the level of relevance. Below is\n",
      "the complete list of available movies, along with their features. Describe these\n",
      "attributes in an appealing manner, commencing with the movie names. The total\n",
      "list of movies includes:[ { \"Movie Title\": \"Daring Odyssey\",    \"Category\":\n",
      "\"Adventure\",   \"Description\": \"Embark on an exciting journey filled with twists\n",
      "and turns. Join the protagonists as they navigate through breathtaking\n",
      "landscapes and face thrilling challenges. A cinematic adventure you won't want\n",
      "to miss!\",},,  …(whole list)...]Prompt 2: You are a language model that\n",
      "recommends movies. Your job is to determine if the movie category and\n",
      "description are the both suitable for the user's freely written\n",
      "description:----User's description: {user_description}. Movie category:\n",
      "{movie_category}Movie description: {movie_description}----Reply with a JSON\n",
      "object in this format:\"reasoning\": Briefly explain your reasoning to the\n",
      "user.\"match\": \"YES\" | \"NO\"Which is a simpler task?3) Power of an Elephant but\n",
      "Memory of a BeeChallenge: While LLMs can handle a broad spectrum of problems,\n",
      "they often face difficulty consistently recalling specific instructions. While\n",
      "their context grows, their ability to retain such details decreases.Solution:\n",
      "Reduce dependency on the LLM's memory by managing internal states within our\n",
      "applications. Generate controlled outputs, transitioning through intermediate\n",
      "steps to meet checkpoints and conditions.Example: This insight came from our\n",
      "experience developing a medical appointment scheduling app. Our first approach\n",
      "involved allowing the model to verify all patient-specified conditions against a\n",
      "list of available specialties and time slots of each doctor, emphasizing the\n",
      "fact that adding non-listed specialties or time slots was prohibited. However,\n",
      "this strategy led to the LLM systematically generating custom specialties not\n",
      "present in the list and unavailable time slots, even after we had already\n",
      "pointed out the mistake in the same conversation. As a solution, we refined our\n",
      "approach by orchestrating the process in validated steps (states): we first\n",
      "leveraged the LLM to collect patient information, such as the reason for\n",
      "consultation and available dates. Following validation, the LLM performed\n",
      "individual matches between the reason and each possible specialty. Subsequently,\n",
      "we employed a traditional database process to filter doctors based on the\n",
      "corresponding specialty and availability on the desired dates. This step ensured\n",
      "that we overcame the issue of inventing non-existing options. Finally, the LLM\n",
      "presented the patient with available appointments until one was\n",
      "confirmed.Simplified diagram of states(light blue) for our medical appointment\n",
      "scheduling application. First managing the learning of user information and\n",
      "after it matching the right specialty and filtering possible doctors and time\n",
      "slots from a database4) Evaluating performance Challenge: Measuring performance\n",
      "and getting unbiased metrics on the quality of text generations. Solution:\n",
      "Selecting a standarized tool for testing and evaluating LLMsExample: This was a\n",
      "clear issue we faced when implementing our medical appointment scheduling\n",
      "application. As a first approach we did a test suite of possible prompts\n",
      "corresponding to a set of topics. Referring to consultations that should get an\n",
      "appointment, others that were not able to be fulfilled due to the lack of a\n",
      "doctor with that specialty, and lastly incorporating messages that should be\n",
      "ignored or moderated because of being malicious. The way to evaluate the test\n",
      "suite was to manually perform the tests by a group of testers and make them\n",
      "provide a satisfaction score from 1 to 5. This was time consuming and tied to\n",
      "the bias of the annotators. We understood that this was not the correct approach\n",
      "for a production environment and did some extra research until we discovered a\n",
      "useful tool: LangSmith. This is a platform that provides support for developing\n",
      "LLM applications oriented to the production environment. It facilitates the test\n",
      "and evaluation process and provides a standard for certain metrics. Moreover, it\n",
      "covers the monitoring and debugging of your application, which makes the whole\n",
      "app lifecycle much more efficient.As an example of our use case, we discovered\n",
      "it valuable to confirm that the successful messages, which offer assistance to\n",
      "the user, included specific information like the patient's name, the described\n",
      "issue, and the doctor's name.LangSmith offers a user-friendly and\n",
      "straightforward approach to implement this functionality, along with\n",
      "comprehensive metrics for the dataset run, including mean, standard deviation,\n",
      "and percentiles of scores and execution time. In this instance, drawing\n",
      "inspiration from the examples outlined in the LangChain Custom Evaluator\n",
      "section, we developed a \"Criteria\" evaluator utilizing an LLM to verify the\n",
      "presence of the specified input values in the response. With just 50 lines of\n",
      "Python code, we were all set to proceed!To illustrate the example, consider the\n",
      "following LLM task…Langchain code running a simple response task for a patient,\n",
      "including some input fieldsNow let’s implement a criteria evaluator for our\n",
      "test:Evaluator to perform the test for each sample in our datasetFinally let’s\n",
      "use the LangSmith client to run the test for each sample in a dataset:LangSmith\n",
      "client code to run the test on a previously created key-value datasetLet’s check\n",
      "how LangSmith’s Web UI registers the trace of the run for one specific\n",
      "sample:The out-of-the-box CriteriaEvalChain returns a score of 1 if the criteria\n",
      "was met, 0 otherwiseCriteriaEvalChain prompt to evaluate the criteriaAmazing! In\n",
      "this way we can run independent tests for specific datasets with custom criteria\n",
      "and have real metrics about the performance of our features with just some lines\n",
      "of code. 5) Security RisksChallenge: Ensuring the reliability of your\n",
      "application to mitigate potential legal issues.Solution: Maintain strict control\n",
      "over both input and output within your application. Avoid direct utilization of\n",
      "the LLM to respond to user input. Instead, break down your processes into\n",
      "independent tasks or states and employ controlled prompts to leverage only the\n",
      "specific capabilities required from the LLM.Example: We consistently integrate\n",
      "an input filter to validate and categorize user input. If a user's intention\n",
      "diverges from the intended purpose of the application, we gently remind them of\n",
      "the app's designated usage. This approach helps us circumvent potential\n",
      "pitfalls, as illustrated by the famous example attached below.Famous pitfall of\n",
      "a company chatbot giving too much freedom to an LLMConclusionTo wrap things up,\n",
      "our exploration into challenges and solutions surrounding the application of\n",
      "Large Language Models (LLMs) for text generation underscores the importance of\n",
      "strategic considerations. By emphasizing the clarity of prompts, task\n",
      "segmentation, and robust evaluation practices, we pave the way for effective LLM\n",
      "applications. The practical solutions presented, including the breakdown of\n",
      "complex tasks, management of internal states to reduce reliance on the LLM's\n",
      "memory, and stringent control over inputs and outputs, provide actionable\n",
      "insights for building reliable and efficient AI text applications.It is evident\n",
      "that a thoughtful approach to LLM integration, grounded in practical\n",
      "methodologies, ensures a more seamless and impactful utilization of these\n",
      "powerful language models. As we refine our approaches, the landscape of\n",
      "possibilities in generative AI for usable text continues to expand, promising\n",
      "exciting developments on the horizon. The ongoing exploration of the synergy\n",
      "between human creativity and AI computational prowess remains at the forefront\n",
      "of this transformative field.   Tags By LangChain   Join our newsletter Updates\n",
      "from the LangChain team and community   Enter your email  Subscribe  Processing\n",
      "your application... Success! Please check your inbox and click the link to\n",
      "confirm your subscription. Sorry, something went wrong. Please try again.\n",
      "You might also like           LangChain State of AI 2024 Report   By LangChain 6\n",
      "min read             Introducing OpenTelemetry support for LangSmith   By\n",
      "LangChain 4 min read             Easier evaluations with LangSmith SDK v0.2   By\n",
      "LangChain 4 min read             LangGraph Platform: New deployment options for\n",
      "scalable agent infrastructure   By LangChain 4 min read             Few-shot\n",
      "prompting to improve tool-calling performance   By LangChain 8 min read\n",
      "Improving core tool interfaces and docs in LangChain   By LangChain 4 min read\n",
      "Sign up                  © LangChain Blog 2025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INDEXING STEP 1: Load Data\n",
    "URL = \"https://blog.langchain.dev/generating-usable-text-with-ai/\"\n",
    "loader = WebBaseLoader(URL)\n",
    "blog_data = loader.load()\n",
    "\n",
    "metadata = blog_data[0].metadata\n",
    "page_content = blog_data[0].page_content\n",
    "\n",
    "print(f\"Loaded {len(blog_data)} document(s).\")\n",
    "print(\"Document metadata:\", metadata)\n",
    "print(f\"Page content length: {len(page_content)} characters.\")\n",
    "print(\"Document content (excerpt):\")\n",
    "print(\"\\n\".join(textwrap.wrap(page_content, width=80)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data is split into 17 smaller chunks.\n",
      "Type of each split: <class 'langchain_core.documents.base.Document'>\n",
      "\n",
      "Content of one of the chunks (excerpt):\n",
      "presented the patient with available appointments until one was confirmed.Simplified diagram of\n",
      "states(light blue) for our medical appointment scheduling application. First managing the learning\n",
      "of user information and after it matching the right specialty and filtering possible doctors and\n",
      "time slots from a database4) Evaluating performance Challenge: Measuring performance and getting\n",
      "unbiased metrics on the quality of text generations. Solution: Selecting a standarized tool for\n",
      "testing and evaluating LLMsExample: This was a clear issue we faced when implementing our medical\n",
      "appointment scheduling application. As a first approach we did a test suite of possible prompts\n",
      "corresponding to a set of topics. Referring to consultations that should get an appointment, others\n",
      "that were not able to be fulfilled due to the lack of a doctor with that specialty, and lastly\n",
      "incorporating messages that should be ignored or moderated because of being malicious. The way to\n",
      "evaluate the test suite was\n"
     ]
    }
   ],
   "source": [
    "# INDEXING STEP 2: Splitting\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "splits = text_splitter.split_documents(blog_data)\n",
    "\n",
    "print(f\"\\nLoaded data is split into {len(splits)} smaller chunks.\")\n",
    "print(f\"Type of each split: {type(splits[0])}\")\n",
    "print(\"\\nContent of one of the chunks (excerpt):\")\n",
    "print(\"\\n\".join(textwrap.wrap(splits[10].page_content, width=100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embedding the first 40 characters of the 10th split:\n",
      "Text: presented the patient with available app\n",
      "Embedding (first 10 dimensions): [0.023445388302206993, -0.043753959238529205, -0.036080505698919296, -0.05090802535414696, -0.015418066643178463, -0.0016566633712500334, 0.015300992876291275, -0.012591246515512466, 0.021811315789818764, 0.0022387499921023846]\n"
     ]
    }
   ],
   "source": [
    "# INDEXING STEP 3: Embedding\n",
    "embeddings = HuggingFaceEmbeddings()\n",
    "print(\"\\nEmbedding the first 40 characters of the 10th split:\")\n",
    "print(\"Text:\", splits[10].page_content[:40])\n",
    "print(\"Embedding (first 10 dimensions):\", embeddings.embed_query(splits[10].page_content[:40])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Response:\n",
      "  The challenges of generating texts with AI include the need for clear and\n",
      "specific prompts, the difficulty of managing internal states to reduce reliance\n",
      "on the LLM's memory, and the importance of controlling inputs and outputs to\n",
      "avoid potential pitfalls. Additionally, the text highlights the need for\n",
      "strategic considerations, such as task segmentation and robust evaluation\n",
      "practices, to ensure effective LLM applications.\n"
     ]
    }
   ],
   "source": [
    "# INDEXING STEP 4: Storing\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=HuggingFaceEmbeddings())\n",
    "\n",
    "# Retrieval and Generation STEP 1: Retrieval\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Retrieval and Generation STEP 2: Querying\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | prompt | llm\n",
    "\n",
    "# Query example\n",
    "response = rag_chain.invoke(\"What are the challenges of generating texts with AI?\")\n",
    "print(\"\\nResponse:\")\n",
    "print(\"\\n\".join(textwrap.wrap(response.content, width=80)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
